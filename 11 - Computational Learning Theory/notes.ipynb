{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Learning Theory\n",
    "\n",
    "Several fundamental questions arise when designing and analyzing algorithms that learn from examples:\n",
    "\n",
    "- What can be learned efficiently? \n",
    "- What is inherently hard to learn? \n",
    "- How many examples are needed to learn successfully? \n",
    "- Is there a general model of learning?\n",
    "\n",
    "To formalize and address these questions, we can consider the **Probably Approximately Correct (PAC)** learning framework, that helps to define the **class of learnable concepts** in terms of the **number of sample needed** to achieve an approximate solution and the **time and space complexity** of the learning algorithm, which depends on the cost of the computational representation of the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Problem\n",
    "\n",
    "We focus on the problem of learning an unknown target function, given only training examples of this target function and a space of candidate hypotheses. Our goal is to answer questions such as:\n",
    "\n",
    "- **sample complexity**: how many examples are needed for a learner to converge (with high probability) to a successful hypothesis?\n",
    "\n",
    "-  **computational complexity**: how much computational effort is needed for a learner to converge (with high probability) to a successful hypothesis?\n",
    "\n",
    "- **mistake bound**: how many examples will the learner misclassify before converging to a successful hypothesis?\n",
    "\n",
    "Notice there are **many specific settings in which we could pursue such questions**. For example, there are various ways to specify **what it means for the learner to be successful**. We might specify that to succeed, the learner must output a hypothesis identical to the target concept. Alternatively, we might simply require that it output a hypothesis that agrees with the target concept most of the time, or that it usually output such a hypothesis. Similarly, we must specify how training examples are to be obtained by the learner. We might specify that training examples\n",
    "are presented by a helpful teacher, or obtained by the learner performing experiments, or simply generated at random according to some process outside the learner's control. As we might expect, the answers to the above questions depend on the particular setting, or learning model, we have in mind.\n",
    "\n",
    "Let define formally the problem setting:\n",
    "\n",
    "- $X$ refer to the **set of all possible instances** over which target functions may be defined\n",
    "- $C$ refer to the **set of target concepts** that our learner might be called upon to learn, which might not necessarily coincide with C\n",
    "\n",
    "Each target concept corresponds to some boolean-valued function: \n",
    "\n",
    "$\\displaystyle c : X -> [0, 1] \\quad c \\in C$\n",
    "\n",
    "For the sake of simplicity, we restrict the discussion to the case of **learning boolean valued\n",
    "concepts from noise-free training data**. However, many of the results can be extended to the more general scenario of learning real-valued target functions and some can be extended to learning from\n",
    "noisy data.\n",
    "\n",
    "We assume instances are **generated at random** from the set of possible instances **according to some probability distribution**, that is  in general not known to the learner.  All that we require is that it be **stationary** (not change over time). Training examples are generated by drawing instances at random according to the distribution, then presenting them along with its target value \n",
    "\n",
    "- $D$ refer to the stationary probability distribution of training examples\n",
    "- $x \\in X$ refer to a training example drawn from $D$\n",
    "- $T$ refer to the set of all training examples\n",
    "- $c(x)$ refer to the target value\n",
    "\n",
    "The learner considers a **set of possible hypotheses** when attempting to learn the target concept. After observing a sequence of training examples of the target concept, it must output some hypothesis, which is its estimate of the target concept. \n",
    "\n",
    "- $L$ refer to the learner\n",
    "- $H$ refer to the set of hypothesis, which might not necessarily coincide with $C$\n",
    "\n",
    "To be fair, we evaluate the success of a learner by the performance of its hypothesi over new instances drawn randomly according to the same probability distribution used to generate the training data.\n",
    "\n",
    "Within this setting, because we demand the learner to be **general enough to learn any target concept**, regardless of the distribution of training examples, we will often be interested in **worst-case analyses** over all possible target concepts and all possible instance distributions.\n",
    "\n",
    "Because we are interested in how closely the learner hypothesi approximates the actual target concept, we need to define **the error of an hypothesis with respect to a target concept and instance distribution**:\n",
    "\n",
    "$\\displaystyle \\text{error}_{D}(h) =   P_{x \\in D}[h(x) \\neq c(x)]$\n",
    "\n",
    "Informally, the error of is **the probability that the hypothesis will misclassify an instances drawn at random according to the probability distribution**. We have chosen to define error over the entire distribution of instances, and not simply over the training examples, because this is the **true error** we expect to encounter when actually using the learned hypothesis on subsequent instances. This error is **not directly observable to the learner**. It can only observe the performance over the training examples, and it must choose its hypothesis on this basis only. \n",
    "\n",
    "$\\displaystyle \\text{error}_{T}(h) =   P_{x \\in T}[c(x) \\neq h(x)]$\n",
    "\n",
    "We will use the term **training error** to refer to the fraction of training examples misclassified, in contrast to the **true error**. Much of our analysis of the complexity of learning centers around the question \"how probable is it that the observed training error gives a misleading estimate of the true error?\"\n",
    "\n",
    "![](images/instance-space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that **error depends strongly on the probability distribution**. For example, if it is a uniform, then the error will be the fraction of the total instance space that falls into the region where hypothesis and target function disagree. However, the error can be higher if the distribution\n",
    "assign high probability to those instances. In the extreme, if the distribution assign zero probability to the instances for which hypothesis and target agree, then the error will be 1, despite the fact that them agree on a large number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Learnability\n",
    "\n",
    "Our aim is to characterize **classes of concepts that can be reliably learned** from a **reasonable number of training examples** and a **reasonable amount of computation**. We might try to characterize the number of training examples needed to learn a hypothesis for which error is zero. Unfortunately, this is futile in the setting we are considering, for two reasons. First, unless we provide training examples corresponding to every possible instance (unrealistic assumption), there may be **multiple hypotheses consistent with the provided training examples**, and the learner cannot be certain to pick the one corresponding to the target concept. Second, given that the training examples are drawn randomly, there will always be some non-zero probability that the training examples encountered by the learner will be **misleading**. To accommodate these two difficulties, we will not require that the learner output a zero error hypothesis, but only that **its error be bounded by some constant** that can be made arbitrarily small. Moreover, we will not require that the learner succeed for every sequence of randomly drawn training examples, but only that **its probability of failure be bounded by some constant** that can be made arbitrarily small. In short, we require only that the learner **probably learn an hypothesis that is approximately correct**, hence the name **PAC: Probably Approximately Correct learning**. More formally, we can provide the following definition:\n",
    "\n",
    "*A concept class $C$, defined over a set of instances $X$ of length $n$, is **PAC-learnable** by a learner $L$ using an hypothesis space $H$ if, for all target concept $c \\in C$ and distribution $D$ over $X$, the learner $L$, with probability $1-\\delta$, will output an hypothesis $h \\in H$ with $error_{x \\in D}(h) < \\epsilon$, after observing a reasonable number of training examples and performing a reasonable amount of computation (in a time polynomial in ${1}/{\\epsilon}$, ${1}/{\\delta}$, $n$ and size of $c$)*\n",
    "\n",
    "This definition requires two things from the learner. First, **it must, with arbitrarily high probability, output a hypothesis having arbitrarily low error**. Second, **it must do efficiently in time** (growing at most polynomially with the strength of our demands on the output hypothesis and with the inherent complexity of the underlying instance space and concept class).  This definition may at first appear to be concerned only with the computational resources required for learning, whereas in practice we are usually more concerned with the number of training examples required. However, the two are very closely related: if the learner requires some minimum processing time per training example, then for the concept class to be PAC-learnable by the learner, the learner must learn from a polynomial number of training examples. A typical approach to show that a concept class is PAC-learnable, is to first show that each target concept can be learned from a polynomial number of training examples and then show that the processing time per example is also polynomially bounded.\n",
    "\n",
    "We should point out a **restrictive assumption** implicit in this definition. We are assuming that the hypothesis space contains an hypothesis with arbitrarily small error for every target concept. Of\n",
    "course this is difficult to assure if one does not know the concept class in advance. Nevertheless, results based on the PAC learning model provide useful insights regarding the relative complexity of different learning problems and regarding the rate at which generalization accuracy improves with additional training examples. Furthermore, it is possible to remove this restrictive assumption, to consider the case in which the learner makes no prior assumption about the form of the target concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample complexity for finite hypothesis spaces\n",
    "\n",
    "We call **sample complexity** the growth in the number of required training examples with problem size. This is really interesting, because in several practical settings, the factor that most limits success of the learner is the **limited availability of training data**. We can derive **a general bound on the sample complexity** for a very broad class of learners, called **consistent learners**, which are learners that output hypotheses that **perfectly fit the training data**. First of all we define the **version space** as the set of all hypotheses that correctly classify the training\n",
    "examples:\n",
    "\n",
    "$\\displaystyle VS_{H,D} = \\{ h \\in H : \\forall x \\in D, h(x) = c(x) \\} $\n",
    "\n",
    "The significance of the version space is that **every consistent learner outputs hypotheses belonging to the version space**. \n",
    "\n",
    "![](images/version-space.png)\n",
    "\n",
    "Therefore, to bound the number of examples needed by any consistent learner, we need to bound the number of examples needed to assure that the version space contains no unacceptable hypotheses. Precisely we can write that the version space is **exhausted** if **all the hypotheses consistent with the observed training examples (i.e., those with zero training error) happen to have true error less than a quantity arbitrary small**:\n",
    "\n",
    "$\\displaystyle \\forall h \\in VS_{H,D} error_{D}(h) < \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only an observer who knew the identity of the target concept could determine with certainty whether the version space is exhausted or not, however, we can bound the probability that the version space will be exhausted after a given number of training examples. If **the hypothesis space is finite** and we have a sequence of $m$ independent randomly drawn example of the target concept, than the probability that the version space is not exhausted is bounded by:\n",
    "\n",
    "$\\displaystyle P(VS_{H,D} \\text{ not exhausted })  \\leq |H| e^{-\\epsilon m}$\n",
    "\n",
    "We can prove this result by considering \n",
    "\n",
    "$\\displaystyle h_1, h_2, \\ldots, h_k$ \n",
    "\n",
    "to be all the hypotheses that have true error greater than $\\epsilon$. We fail to exhaust the version space if and only if at least one of these hypotheses is consistent with all the m training examples. The probability that any single of  these hypothesis would be consistent with one randomly drawn example is \n",
    "\n",
    "$\\displaystyle1 - \\epsilon$\n",
    "\n",
    "Therefore the probability that this hypothesis will be consistent with all $m$ examples is at most\n",
    "\n",
    "$\\displaystyle (1-\\epsilon)^m$\n",
    "\n",
    "Given that the number of such hypothesis is k, the probability that at least one of them will be consistent with all the examples is at most \n",
    "\n",
    "$\\displaystyle k(1-\\epsilon)^m$. \n",
    " \n",
    "Since the number of hypotheses is finite, then \n",
    "\n",
    "$\\displaystyle k \\leq |H|$ \n",
    "\n",
    "and the probability becomes al most \n",
    "\n",
    "$\\displaystyle |H|(1-\\epsilon)^m$. \n",
    "\n",
    "Finally we use the general inequality \n",
    "\n",
    "$\\displaystyle 1-\\epsilon \\leq e^{-\\epsilon}$ \n",
    "\n",
    "to obtain the desired result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **bounds the probability that the training examples fail to eliminate all bad hypotheses**(hypotheses with true error greater than a certain amount) for any consistent learner using the hypothesis space H. We can use this result to **determine the number of training examples required\n",
    "to reduce this probability of failure below some desired level**:\n",
    "\n",
    "$\\displaystyle |H| e^{-\\epsilon m} \\leq \\delta $\n",
    "\n",
    "rearranging we obtain the number of training examples sufficient for any consistent learner to successfully learn any target concept:\n",
    "\n",
    "$\\displaystyle m \\geq \\frac{1}{\\epsilon} \\left( \\ln |H| + \\ln \\frac{1}{\\delta} \\right)$\n",
    "\n",
    "This number of training examples is sufficient to assure that any consistent hypothesis will be **probably** (with probability $1 - \\delta$) **approximately** (within error $\\epsilon$) **correct**. \n",
    "\n",
    "Notice that the number of training examples:\n",
    "\n",
    "- grows linearly in ${1}/{\\epsilon}$\n",
    "- grows logarithmically in ${1}/{\\delta}$\n",
    "- grows logarithmically in the size of the hypothesis space\n",
    "\n",
    "The above bound can be **a substantial overestimate**, it grows linearly in the size of the hypothesis space, which can be very large (and even infinite). However, we can observe that in general the **size of the hypothesis space grows exponentially with the number of attributes** and this is the reason why it is difficult to learn from a large number of attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in the most general case, there may be no hypothesis consistent with the labeled\n",
    "training sample. This, in fact, is the typical case in practice, where the learning problems may be somewhat difficult or the concept classes more complex than the hypothesis set used by the learning algorithm. However, inconsistent hypotheses with a small number of errors on the training sample can be useful. In this case, the most we might ask is the hypothesis that has **the minimum error over the training examples**. A learner that makes no assumption that the target concept is representable by H and that simply finds the hypothesis with minimum training error, is often called an **agnostic learner**, because it makes no prior commitment about the relationship between C and H. We can derive a bound also for this more general case. To be more precise we can consider the hypothesis having lowest training error over the training examples:\n",
    "\n",
    "$\\displaystyle h_{best} = \\arg \\min_{h \\in H} error_{T}(h)$\n",
    "\n",
    "Notice that the training error may differ from the true error error, over the entire probability distribution. And we can ask how many training examples suffice to ensure with high probability that the true error error will be no more than some quantity bigger than the training error? \n",
    "\n",
    "$\\displaystyle error_{D}(h_{best}) \\leq error_{T}(h_{best}) + \\epsilon$\n",
    "\n",
    "Notice the question considered previously is just a special case of this question, when the training error happens to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question can be answered in a similar way, however we need to consider the general **Hoeffding bound**, that characterize the deviation between the true probability of some event and its observed\n",
    "frequency over several independent trials:\n",
    "\n",
    "$\\displaystyle P(error_{D}(h) > error_{T}(h) + \\epsilon) \\leq e^{-2 m \\epsilon^2}$\n",
    "\n",
    "This gives us a bound on the probability that an arbitrarily chosen single hypothesis has a very misleading training error. To assure that the best hypothesis found by the learner has an error bounded in this way, we must consider the probability that any one of the hypotheses could have a large error:\n",
    "\n",
    "$\\displaystyle P(\\exists h \\in H : error_{D}(h) > error_{T}(h) + \\epsilon) \\leq |H| e^{-2 m \\epsilon^2}$\n",
    "\n",
    "If we call this probability $\\delta$, and ask how many example suffice to hold this probability below some desired level, we obtain the following bound:\n",
    "\n",
    "$\\displaystyle m \\geq \\frac{1}{2 \\epsilon^2} \\left( \\ln |H| + \\ln \\frac{1}{\\delta} \\right)$\n",
    "\n",
    "This generalize the previous bound to the case in which the learner still picks the best hypothesis, but where the best hypothesis may have non-zero training error. Notice that m, in this less restrictive situation grows as the square of ${1}/{\\epsilon}$, rather than linearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Conjunctions of boolean literals\n",
    "\n",
    "Consider learning the concept class of conjunctions of at most $n$ boolean literals: \n",
    "\n",
    "$\\displaystyle x_1, x_2, \\ldots, x_n$\n",
    "\n",
    "where each boolean literal is either a boolean variable $x_i$ or its negation. If n=4, an example of conjunction can be:\n",
    "\n",
    "$\\displaystyle x_1 \\> \\cdot \\> \\bar{x}_2  \\> \\cdot \\> x_4$\n",
    "\n",
    "A positive sample is:\n",
    "\n",
    "$\\displaystyle x_1 = 1, x_2 = 0, x_3 = 0, x_4 = 1$\n",
    "\n",
    "a negative sample is:\n",
    "\n",
    "$\\displaystyle x_1 = 1, x_2 = 0, x_3 = 1, x_4 = 0$\n",
    "\n",
    "Is it PAC-learnable? In order to answer this question, first we can show that any consistent learner will require only a polynomial number of training examples to learn any concept (using the bound), then we propose a specific algorithm that uses polynomial time per training example.\n",
    "\n",
    "Consider any consistent learner using a hypothesis space identical to the concept space. We can use the first bound to compute the number of training examples sufficient to ensure that the learner will, with some probability, output a hypothesis with a maximum error E. To accomplish this, we need only determine the size of the hypothesis space. For example, we can consider the conjunctions of literals based on n boolean variables, in that case the size of the hypothesis space is $3^n$ (there are only three possibilities for each variable in any given hypothesis: include the variable, include its negation, or ignore it). Therefore, the number of training examples is bounded by:\n",
    "\n",
    "$\\displaystyle m \\geq \\frac{1}{\\epsilon} \\left( \\ln 3^n + \\ln \\frac{1}{\\delta} \\right) = \\frac{1}{\\epsilon} \\left( n \\ln 3 + \\ln \\frac{1}{\\delta} \\right)$\n",
    "\n",
    "For example, if a consistent learner attempts to learn a target concept described by conjunctions of 10 boolean literals, and we desire a 0.95 probability that it will learn a hypothesis with error less than 0.1, then we need to randomly drawn a number of training examples:\n",
    "\n",
    "$\\displaystyle m \\geq \\frac{1}{0.1} \\left( 10 \\ln 3 + \\ln \\frac{1}{0.05} \\right) = 140$\n",
    "\n",
    "Notice that the number of needed example grows linearly in the number of literals n, linearly in ${1}/{\\epsilon}$, logarithmically in ${1}/{\\delta}$ and it si independent of size (c).\n",
    "\n",
    "Now we need to specify an algorithm and show that it requires no more than polynomial computation per training example. We can observe from the previous example, that a positive example (1, 0, 1, 0) implies that the target concept cannot contain the negation of the variables $x_1$ and $x_3$ and that it cannot contain the variables $x_2$ and $x_4$ in positive form. In contrast, negative examples are less informative, since it is not known which of their bits are incorrect. Thus a simple algorithm for finding a consistent hypothesis can be based on positive examples: \n",
    "\n",
    "For each positive example \n",
    "\n",
    "$\\displaystyle b_1, b_2, \\ldots, b_n$\n",
    "\n",
    "- if $b_i = 1$ then $\\bar{x}_i$ is ruled out as a possible literal in the concept class \n",
    "- if $b_i = 0$ then $x_i$ is ruled out. \n",
    "\n",
    "The conjunction of all the literals not ruled out is an hypothesis consistent with the target. For each new positive training example, this algorithm use time linear in n. The following figure shows an example for the case n=6:\n",
    "\n",
    "![](images/conjunction-boolean-literals.png)\n",
    "\n",
    "In conclusion, since the proposed algorithm requires no more than polynomial computation per training example, and all consistent learners requires no more than a polynomial number of training examples, then the total computation required will be polynomial as well and the problem is PAC-learnable.\n",
    "\n",
    "Of course, not all concept classes have polynomially bounded sample complexity and it is also possible to find concept classes that have polynomial sample complexity, but nevertheless cannot be learned in polynomial time because there is no polynomial time algorithm that can learn them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:  Universal concept class\n",
    "\n",
    "Consider the set of all boolean vectors with n components (all possible sequences of 0 and 1, each of length n):\n",
    "\n",
    "$\\displaystyle X =  \\{0, 1\\}^n$\n",
    "\n",
    "For example, if $n=2$, then $X$ would contain \n",
    "\n",
    "$\\displaystyle \\{00, 01, 10, 11\\}$\n",
    "\n",
    "The number of possible elements in X is $2^n$. Then consider the concept class $U_n$, formed by all subset of X. In the example with $n=2$, the concept class would contain:\n",
    "\n",
    "$\\displaystyle \\{00\\}, \\{01\\}, \\{10\\}, \\{11\\},$\n",
    "\n",
    "$\\displaystyle \\{00, 01\\}, \\{00, 10\\}, \\{00, 11\\}, \\{01, 10\\}, \\{01, 11\\}, \\{10, 11\\},$\n",
    "\n",
    "$\\displaystyle \\{00, 01, 10\\}, \\{00, 01, 11\\}, \\{00, 10, 11\\}, \\{01, 10, 11\\},$\n",
    "\n",
    "$\\displaystyle \\{00, 01, 10, 11\\}$\n",
    "\n",
    "Is this class PAC-learnable? To guarantee a consistent hypothesis, the hypothesis class must include the concept class, thus $|H| \\geq |U_n| and the number of possible concepts is $2^{2^n}$, which is exponential in $n$. Therefore the number of training examples required is: \n",
    "\n",
    "$\\displaystyle m \\geq \\frac{1}{\\epsilon} \\left( \\ln 2^{2^n} + \\ln \\frac{1}{\\delta} \\right) = \\frac{1}{\\epsilon} \\left( 2^n \\ln 2 + \\ln \\frac{1}{\\delta} \\right)$\n",
    "\n",
    "This number of training examples grows exponentially in n and the concept class is not PAC-learnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample complexity for infinite hypothesis spaces\n",
    "\n",
    "The hypothesis sets typically used in machine learning are **infinite**. In this case, the bound on the number of training examples required to assure that the version space is not exhausted is not directly applicable because $|H|= \\infty$. Here we consider a second measure of the complexity of the hypotheis space, called the **Vapnik-Chervonenkis VC** dimension, which measures the complexity of the hypothesis space H, not by the number of distinct hypotheses $|H|$, but instead by **the number of distinct instances from X that can be completely discriminated using H**.\n",
    "\n",
    "\n",
    "XXX\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
